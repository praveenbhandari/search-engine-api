{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_term_weights( query):\n",
    "    \"\"\"Predict term weights for the given query using BERT embeddings.\"\"\"\n",
    "    # Tokenize and encode the query for BERT\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    # print(inputs)\n",
    "    # Get embeddings from BERT model\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use the last hidden state\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "    # print(len(last_hidden_states[0][0]))\n",
    "    # print((last_hidden_states[0][0]))\n",
    "    # Simplified weight prediction logic: Use the norm of the embeddings as weights\n",
    "    # This is a placeholder and should be replaced with a proper mechanism\n",
    "    weights = torch.norm(last_hidden_states, dim=-1).squeeze().tolist()\n",
    "    # print(weights)\n",
    "\n",
    "    # Associate weights with tokens. This assumes no special tokens (CLS, SEP) for simplicity.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    return dict(zip(tokens, weights))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "corpus = [\n",
    "    \"Hello there good man!\",\n",
    "    \"Hello there good man!\",\n",
    "    \"It is quite windy in London\",\n",
    "    \"hello how is the weather today?\"\n",
    "]\n",
    "\n",
    "tokenized_corpus = [doc.split(\" \") for doc in corpus]\n",
    "\n",
    "bm25 = BM25Okapi(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"hello man\"\n",
    "tokenized_query = query.split(\" \")\n",
    "\n",
    "doc_scores = bm25.get_scores(tokenized_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello how is the weather today?']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm25.get_top_n(tokenized_query, corpus, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import re\n",
    "import operator\n",
    "import nltk \n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "      <td>13</td>\n",
       "      <td>sci.med</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 content  target   \n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...       7  \\\n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...       4   \n",
       "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...       4   \n",
       "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...       1   \n",
       "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...      14   \n",
       "...                                                  ...     ...   \n",
       "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...      13   \n",
       "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...       4   \n",
       "11311  From: westes@netcom.com (Will Estes)\\nSubject:...       3   \n",
       "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...       1   \n",
       "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...       8   \n",
       "\n",
       "                   target_names  \n",
       "0                     rec.autos  \n",
       "1         comp.sys.mac.hardware  \n",
       "2         comp.sys.mac.hardware  \n",
       "3                 comp.graphics  \n",
       "4                     sci.space  \n",
       "...                         ...  \n",
       "11309                   sci.med  \n",
       "11310     comp.sys.mac.hardware  \n",
       "11311  comp.sys.ibm.pc.hardware  \n",
       "11312             comp.graphics  \n",
       "11313           rec.motorcycles  \n",
       "\n",
       "[11314 rows x 3 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_json('https://raw.githubusercontent.com/zayedrais/DocumentSearchEngine/master/data/newsgroups.json')\n",
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0  WHAT car is this!?\\n</td>\n",
       "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1  SI Clock Poll - Final Call\\n</td>\n",
       "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2  PB questions...\\n</td>\n",
       "      <td>From: twillis@ec.ecn.purdue.edu (Thomas E Will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3  Re: Weitek P9000 ?\\n</td>\n",
       "      <td>From: jgreen@amber (Joe Green)\\nSubject: Re: W...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4  Re: Shuttle Launch Question\\n</td>\n",
       "      <td>From: jcm@head-cfa.harvard.edu (Jonathan McDow...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>11309  Re: Migraines and scans\\n</td>\n",
       "      <td>From: jim.zisfein@factory.com (Jim Zisfein) \\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>11310  Screen Death: Mac Plus/512\\n</td>\n",
       "      <td>From: ebodin@pearl.tufts.edu\\nSubject: Screen ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>11311  Mounting CPU Cooler in vertical case\\n</td>\n",
       "      <td>From: westes@netcom.com (Will Estes)\\nSubject:...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>11312  Re: Sphere from 4 points?\\n</td>\n",
       "      <td>From: steve@hcrlgw (Steven Collins)\\nSubject: ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>11313  stolen CBR900RR\\n</td>\n",
       "      <td>From: gunning@cco.caltech.edu (Kevin J. Gunnin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject   \n",
       "0                            0  WHAT car is this!?\\n  \\\n",
       "1                    1  SI Clock Poll - Final Call\\n   \n",
       "2                               2  PB questions...\\n   \n",
       "3                            3  Re: Weitek P9000 ?\\n   \n",
       "4                   4  Re: Shuttle Launch Question\\n   \n",
       "...                                              ...   \n",
       "11309               11309  Re: Migraines and scans\\n   \n",
       "11310            11310  Screen Death: Mac Plus/512\\n   \n",
       "11311  11311  Mounting CPU Cooler in vertical case\\n   \n",
       "11312             11312  Re: Sphere from 4 points?\\n   \n",
       "11313                       11313  stolen CBR900RR\\n   \n",
       "\n",
       "                                                 content  \n",
       "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...  \n",
       "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...  \n",
       "2      From: twillis@ec.ecn.purdue.edu (Thomas E Will...  \n",
       "3      From: jgreen@amber (Joe Green)\\nSubject: Re: W...  \n",
       "4      From: jcm@head-cfa.harvard.edu (Jonathan McDow...  \n",
       "...                                                  ...  \n",
       "11309  From: jim.zisfein@factory.com (Jim Zisfein) \\n...  \n",
       "11310  From: ebodin@pearl.tufts.edu\\nSubject: Screen ...  \n",
       "11311  From: westes@netcom.com (Will Estes)\\nSubject:...  \n",
       "11312  From: steve@hcrlgw (Steven Collins)\\nSubject: ...  \n",
       "11313  From: gunning@cco.caltech.edu (Kevin J. Gunnin...  \n",
       "\n",
       "[11314 rows x 2 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i,txt in enumerate(news['content']):\n",
    "    subject = re.findall('Subject:(.*\\n)',txt)\n",
    "    if (len(subject) !=0):\n",
    "        news.loc[i,'Subject'] =str(i)+' '+subject[0]\n",
    "    else:\n",
    "        news.loc[i,'Subject'] ='NA'\n",
    "df_news =news[['Subject','content']]\n",
    "df_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_news.loc[:, 'content'] = df_news.loc[:, 'content'].replace(to_replace='^(from|lines):(.*)\\n', value='', regex=True,)\n",
    "df_news.loc[:, 'content'] = df_news.loc[:, 'content'].replace(to_replace='[!\"#$%&\\'()*+,/:;<=>?@[\\\\]^_`{|}~]', value=' ', regex=True)\n",
    "df_news.loc[:, 'content'] = df_news.loc[:, 'content'].replace(to_replace='-', value=' ', regex=True)\n",
    "df_news.loc[:, 'content'] = df_news.loc[:, 'content'].replace(to_replace='\\s+', value=' ', regex=True)\n",
    "df_news.loc[:, 'content'] = df_news.loc[:, 'content'].apply(lambda x: x.strip())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Make sure all content is in lowercase\n",
    "df_news.loc[:, 'content'] = df_news.loc[:, 'content'].apply(lambda x: x.lower())\n",
    "\n",
    "# Tokenize the content in the 'content' column\n",
    "df_news.loc[:, 'Word tokenize'] = df_news.loc[:, 'content'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Subject</th>\n",
       "      <th>content</th>\n",
       "      <th>Word tokenize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0  WHAT car is this!?\\n</td>\n",
       "      <td>from lerxst wam.umd.edu where s my thing subje...</td>\n",
       "      <td>[from, lerxst, wam.umd.edu, where, s, my, thin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1  SI Clock Poll - Final Call\\n</td>\n",
       "      <td>from guykuo carson.u.washington.edu guy kuo su...</td>\n",
       "      <td>[from, guykuo, carson.u.washington.edu, guy, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2  PB questions...\\n</td>\n",
       "      <td>from twillis ec.ecn.purdue.edu thomas e willis...</td>\n",
       "      <td>[from, twillis, ec.ecn.purdue.edu, thomas, e, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3  Re: Weitek P9000 ?\\n</td>\n",
       "      <td>from jgreen amber joe green subject re weitek ...</td>\n",
       "      <td>[from, jgreen, amber, joe, green, subject, re,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4  Re: Shuttle Launch Question\\n</td>\n",
       "      <td>from jcm head cfa.harvard.edu jonathan mcdowel...</td>\n",
       "      <td>[from, jcm, head, cfa.harvard.edu, jonathan, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11309</th>\n",
       "      <td>11309  Re: Migraines and scans\\n</td>\n",
       "      <td>from jim.zisfein factory.com jim zisfein subje...</td>\n",
       "      <td>[from, jim.zisfein, factory.com, jim, zisfein,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11310</th>\n",
       "      <td>11310  Screen Death: Mac Plus/512\\n</td>\n",
       "      <td>from ebodin pearl.tufts.edu subject screen dea...</td>\n",
       "      <td>[from, ebodin, pearl.tufts.edu, subject, scree...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11311</th>\n",
       "      <td>11311  Mounting CPU Cooler in vertical case\\n</td>\n",
       "      <td>from westes netcom.com will estes subject moun...</td>\n",
       "      <td>[from, westes, netcom.com, will, estes, subjec...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11312</th>\n",
       "      <td>11312  Re: Sphere from 4 points?\\n</td>\n",
       "      <td>from steve hcrlgw steven collins subject re sp...</td>\n",
       "      <td>[from, steve, hcrlgw, steven, collins, subject...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11313</th>\n",
       "      <td>11313  stolen CBR900RR\\n</td>\n",
       "      <td>from gunning cco.caltech.edu kevin j. gunning ...</td>\n",
       "      <td>[from, gunning, cco.caltech.edu, kevin, j., gu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Subject   \n",
       "0                            0  WHAT car is this!?\\n  \\\n",
       "1                    1  SI Clock Poll - Final Call\\n   \n",
       "2                               2  PB questions...\\n   \n",
       "3                            3  Re: Weitek P9000 ?\\n   \n",
       "4                   4  Re: Shuttle Launch Question\\n   \n",
       "...                                              ...   \n",
       "11309               11309  Re: Migraines and scans\\n   \n",
       "11310            11310  Screen Death: Mac Plus/512\\n   \n",
       "11311  11311  Mounting CPU Cooler in vertical case\\n   \n",
       "11312             11312  Re: Sphere from 4 points?\\n   \n",
       "11313                       11313  stolen CBR900RR\\n   \n",
       "\n",
       "                                                 content   \n",
       "0      from lerxst wam.umd.edu where s my thing subje...  \\\n",
       "1      from guykuo carson.u.washington.edu guy kuo su...   \n",
       "2      from twillis ec.ecn.purdue.edu thomas e willis...   \n",
       "3      from jgreen amber joe green subject re weitek ...   \n",
       "4      from jcm head cfa.harvard.edu jonathan mcdowel...   \n",
       "...                                                  ...   \n",
       "11309  from jim.zisfein factory.com jim zisfein subje...   \n",
       "11310  from ebodin pearl.tufts.edu subject screen dea...   \n",
       "11311  from westes netcom.com will estes subject moun...   \n",
       "11312  from steve hcrlgw steven collins subject re sp...   \n",
       "11313  from gunning cco.caltech.edu kevin j. gunning ...   \n",
       "\n",
       "                                           Word tokenize  \n",
       "0      [from, lerxst, wam.umd.edu, where, s, my, thin...  \n",
       "1      [from, guykuo, carson.u.washington.edu, guy, k...  \n",
       "2      [from, twillis, ec.ecn.purdue.edu, thomas, e, ...  \n",
       "3      [from, jgreen, amber, joe, green, subject, re,...  \n",
       "4      [from, jcm, head, cfa.harvard.edu, jonathan, m...  \n",
       "...                                                  ...  \n",
       "11309  [from, jim.zisfein, factory.com, jim, zisfein,...  \n",
       "11310  [from, ebodin, pearl.tufts.edu, subject, scree...  \n",
       "11311  [from, westes, netcom.com, will, estes, subjec...  \n",
       "11312  [from, steve, hcrlgw, steven, collins, subject...  \n",
       "11313  [from, gunning, cco.caltech.edu, kevin, j., gu...  \n",
       "\n",
       "[11314 rows x 3 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_news\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import operator\n",
    "## Create Vocabulary\n",
    "vocabulary = set()\n",
    "for doc in df_news.content:\n",
    "    vocabulary.update(doc.split(','))\n",
    "vocabulary = list(vocabulary)\n",
    "# Intializating the tfIdf model\n",
    "tfidf = TfidfVectorizer(vocabulary=vocabulary)\n",
    "# Fit the TfIdf model\n",
    "tfidf.fit(df_news.content)\n",
    "# Transform the TfIdf model\n",
    "tfidf_tran=tfidf.transform(df_news.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_vector_T(tokens):\n",
    "    Q = np.zeros((len(vocabulary)))    \n",
    "    x= tfidf.transform(tokens)\n",
    "    #print(tokens[0].split(','))\n",
    "    for token in tokens[0].split(','):\n",
    "        #print(token)\n",
    "        try:\n",
    "            ind = vocabulary.index(token)\n",
    "            Q[ind]  = x[0, tfidf.vocabulary_[token]]\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordLemmatizer(data):\n",
    "    tag_map = defaultdict(lambda : wn.NOUN)\n",
    "    tag_map['J'] = wn.ADJ\n",
    "    tag_map['V'] = wn.VERB\n",
    "    tag_map['R'] = wn.ADV\n",
    "    file_clean_k =pd.DataFrame()\n",
    "    for index,entry in enumerate(data):\n",
    "        \n",
    "        # Declaring Empty List to store the words that follow the rules for this step\n",
    "        Final_words = []\n",
    "        # Initializing WordNetLemmatizer()\n",
    "        word_Lemmatized = WordNetLemmatizer()\n",
    "        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "        for word, tag in pos_tag(entry):\n",
    "            # Below condition is to check for Stop words and consider only alphabets\n",
    "            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n",
    "                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "                Final_words.append(word_Final)\n",
    "            # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                file_clean_k.loc[index,'Keyword_final'] = str(Final_words)\n",
    "                file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n",
    "                file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n",
    "                file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n",
    "                file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n",
    "    return file_clean_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(float(np.linalg.norm(a)*np.linalg.norm(b)))\n",
    "    # print(\"asa\")\n",
    "\n",
    "    \n",
    "    return cos_sim\n",
    "def cosine_similarity_T(k, query):\n",
    "    preprocessed_query  = re.sub(\"\\W+\", \" \", query).strip()\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    q_df = pd.DataFrame(columns=['q_clean'])\n",
    "    q_df.loc[0,'q_clean'] =tokens\n",
    "    q_df['q_clean'] =wordLemmatizer(q_df.q_clean)\n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector_T(q_df['q_clean'])\n",
    "    for d in tfidf_tran.A:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "                    \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "    #print(\"\")\n",
    "    d_cosines.sort()\n",
    "    a = pd.DataFrame()\n",
    "    for i,index in enumerate(out):\n",
    "        a.loc[i,'index'] = str(index)\n",
    "        a.loc[i,'Subject'] = df_news['Subject'][index]\n",
    "    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n",
    "        a.loc[j,'Score'] = simScore\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_4/y16f4k4n1jg0hnnzvrqsl3sw0000gn/T/ipykernel_19371/1581085601.py:2: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  cos_sim = np.dot(a, b)/(float(np.linalg.norm(a)*np.linalg.norm(b)))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11313</td>\n",
       "      <td>11313  stolen CBR900RR\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3712</td>\n",
       "      <td>3712  Re: Drinking and Riding\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3776</td>\n",
       "      <td>3776  RE: was:Go Hezbollah!\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3775</td>\n",
       "      <td>3775  Re: Analog switches/Balan\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3774</td>\n",
       "      <td>3774  Re: A question that has bee bothering me.\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3773</td>\n",
       "      <td>3773  Re: After 2000 years, can we say that Ch...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3772</td>\n",
       "      <td>3772  --- CR-ROM Drive Recommendation? ---\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3771</td>\n",
       "      <td>3771  Re: Windows Help\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3770</td>\n",
       "      <td>3770  Re: AF/ATS: Red Army Fraction (RAF) comm...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3769</td>\n",
       "      <td>3769  Re: With a surge in the last two weeks...\\n</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                            Subject  Score\n",
       "0  11313                           11313  stolen CBR900RR\\n    NaN\n",
       "1   3712                    3712  Re: Drinking and Riding\\n    NaN\n",
       "2   3776                      3776  RE: was:Go Hezbollah!\\n    NaN\n",
       "3   3775                  3775  Re: Analog switches/Balan\\n    NaN\n",
       "4   3774  3774  Re: A question that has bee bothering me.\\n    NaN\n",
       "5   3773  3773  Re: After 2000 years, can we say that Ch...    NaN\n",
       "6   3772       3772  --- CR-ROM Drive Recommendation? ---\\n    NaN\n",
       "7   3771                           3771  Re: Windows Help\\n    NaN\n",
       "8   3770  3770  Re: AF/ATS: Red Army Fraction (RAF) comm...    NaN\n",
       "9   3769  3769  Re: With a surge in the last two weeks...\\n    NaN"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarity_T(10,\"computer science\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/praveenlawyantra/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "import torch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 83\u001b[0m\n\u001b[1;32m     80\u001b[0m bm25 \u001b[38;5;241m=\u001b[39m tokenize_documents(docs)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# predict_term_weights(\"Machine learning teaches machine how to learn\")\u001b[39;00m\n\u001b[0;32m---> 83\u001b[0m search(bm25,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 60\u001b[0m, in \u001b[0;36msearch\u001b[0;34m(b25, query)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# print(scores.items())\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# # print(\"sssss\",weighted_query)\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term, weight \u001b[38;5;129;01min\u001b[39;00m weighted_query:\n\u001b[0;32m---> 60\u001b[0m     term_scores \u001b[38;5;241m=\u001b[39m b25\u001b[38;5;241m.\u001b[39mget_scores([term])\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# print(term_scores)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m doc_id, score \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(term_scores):\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;66;03m# print(doc_id,score)\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get_scores'"
     ]
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "from collections import defaultdict\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# document1 = \"Machine learning teaches machine how to learn\"\n",
    "# document2 = \"i am apple\"\n",
    "# document3 = \"mango i like learning\"\n",
    "# document4 = \"learning is shit\"\n",
    "# docs = [document1, document2, document3, document4]\n",
    "\n",
    "\n",
    "sentences = [\n",
    "    \"Despite the heavy rain that morning, the team decided to proceed with the outdoor charity event, setting up tents and arranging the venue with unwavering determination, hoping their efforts would raise significant funds for the local animal shelter.\",\n",
    "    \"As the sun dipped below the horizon, casting a golden hue across the sky, Maria stood at the edge of the cliff, reflecting on the decisions that had led her to this moment of solitude, feeling a mix of apprehension and excitement about the future.\",\n",
    "    \"The ancient library, with its towering shelves laden with dusty tomes and manuscripts, whispered secrets of a bygone era to those who dared to explore its depths, offering insights into worlds long vanished and knowledge that had been sought by scholars for centuries.\",\n",
    "    \"In the heart of the bustling city, amidst the cacophony of honking cars, chattering pedestrians, and the occasional blare of sirens, there existed a small, serene park where one could escape the relentless pace of urban life and find a moment of tranquility.\",\n",
    "    \"The groundbreaking ceremony, attended by local dignitaries, esteemed guests, and community members, marked the commencement of the construction of the new community center, envisioned as a beacon of hope and unity for the neighborhood's diverse population.\",\n",
    "    \"Eleanor, an accomplished violinist, took to the stage with a grace born of years of practice, her fingers dancing over the strings as she poured her soul into the performance, captivating the audience with a melody that spoke of love, loss, and redemption.\",\n",
    "    \"Amidst the chaos of the battlefield, the young soldier found a moment of clarity, realizing the true cost of war not in terms of territory gained or lost, but in the human lives irrevocably changed by the horrors they had witnessed and the sacrifices they had made.\",\n",
    "    \"The chef, a master of his craft, meticulously combined the freshest local ingredients with exotic spices brought from distant lands, creating a dish that was not only a feast for the palate but also a celebration of the diverse cultures that had influenced his culinary journey.\",\n",
    "    \"As the debate raged on, the professor, armed with decades of research and an unwavering belief in the importance of preserving ancient languages, defended their value not merely as academic curiosities but as vital links to our collective human heritage and windows into the minds of our ancestors.\",\n",
    "    \"The novelist, after years of grappling with writer's block, found inspiration in the least expected of places—a small, unremarkable café where the mosaic of human interactions and the simplicity of everyday moments sparked the idea for her next best-selling novel, weaving together themes of love, resilience, and the beauty of the mundane.\"\n",
    "]\n",
    "\n",
    "# sentences\n",
    "\n",
    "def tokenize_documents(documents):\n",
    "    tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    return bm25\n",
    "\n",
    "def predict_term_weights(query):\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    last_hidden_states = outputs.last_hidden_state\n",
    "    # Placeholder for calculating weights. You need to define this part.\n",
    "    weights = torch.mean(last_hidden_states, dim=1).squeeze().numpy()  # Simplified example\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    return dict(zip(tokens, weights))\n",
    "\n",
    "def search(docs,query):\n",
    "    bm25 = tokenize_documents(docs)\n",
    "    term_weights = predict_term_weights(query)\n",
    "    weighted_query = [(term, weight) for term, weight in term_weights.items()]\n",
    "    \n",
    "    scores = defaultdict(float)\n",
    "    for term, weight in weighted_query:\n",
    "        term_scores = bm25.get_scores([term])\n",
    "        for doc_id, score in enumerate(term_scores):\n",
    "            scores[doc_id] += score * weight\n",
    "    sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return [docs[doc_id] for doc_id, _ in sorted_docs]\n",
    "\n",
    "# Example usage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The ancient library, with its towering shelves laden with dusty tomes and manuscripts, whispered secrets of a bygone era to those who dared to explore its depths, offering insights into worlds long vanished and knowledge that had been sought by scholars for centuries.', 'Despite the heavy rain that morning, the team decided to proceed with the outdoor charity event, setting up tents and arranging the venue with unwavering determination, hoping their efforts would raise significant funds for the local animal shelter.', 'As the sun dipped below the horizon, casting a golden hue across the sky, Maria stood at the edge of the cliff, reflecting on the decisions that had led her to this moment of solitude, feeling a mix of apprehension and excitement about the future.', 'In the heart of the bustling city, amidst the cacophony of honking cars, chattering pedestrians, and the occasional blare of sirens, there existed a small, serene park where one could escape the relentless pace of urban life and find a moment of tranquility.', \"The groundbreaking ceremony, attended by local dignitaries, esteemed guests, and community members, marked the commencement of the construction of the new community center, envisioned as a beacon of hope and unity for the neighborhood's diverse population.\", 'Eleanor, an accomplished violinist, took to the stage with a grace born of years of practice, her fingers dancing over the strings as she poured her soul into the performance, captivating the audience with a melody that spoke of love, loss, and redemption.', 'Amidst the chaos of the battlefield, the young soldier found a moment of clarity, realizing the true cost of war not in terms of territory gained or lost, but in the human lives irrevocably changed by the horrors they had witnessed and the sacrifices they had made.', 'The chef, a master of his craft, meticulously combined the freshest local ingredients with exotic spices brought from distant lands, creating a dish that was not only a feast for the palate but also a celebration of the diverse cultures that had influenced his culinary journey.', 'As the debate raged on, the professor, armed with decades of research and an unwavering belief in the importance of preserving ancient languages, defended their value not merely as academic curiosities but as vital links to our collective human heritage and windows into the minds of our ancestors.', \"The novelist, after years of grappling with writer's block, found inspiration in the least expected of places—a small, unremarkable café where the mosaic of human interactions and the simplicity of everyday moments sparked the idea for her next best-selling novel, weaving together themes of love, resilience, and the beauty of the mundane.\"]\n",
      "\n",
      "The ancient library, with its towering shelves laden with dusty tomes and manuscripts, whispered secrets of a bygone era to those who dared to explore its depths, offering insights into worlds long vanished and knowledge that had been sought by scholars for centuries.\n",
      "\n",
      "Despite the heavy rain that morning, the team decided to proceed with the outdoor charity event, setting up tents and arranging the venue with unwavering determination, hoping their efforts would raise significant funds for the local animal shelter.\n",
      "\n",
      "As the sun dipped below the horizon, casting a golden hue across the sky, Maria stood at the edge of the cliff, reflecting on the decisions that had led her to this moment of solitude, feeling a mix of apprehension and excitement about the future.\n",
      "\n",
      "In the heart of the bustling city, amidst the cacophony of honking cars, chattering pedestrians, and the occasional blare of sirens, there existed a small, serene park where one could escape the relentless pace of urban life and find a moment of tranquility.\n",
      "\n",
      "The groundbreaking ceremony, attended by local dignitaries, esteemed guests, and community members, marked the commencement of the construction of the new community center, envisioned as a beacon of hope and unity for the neighborhood's diverse population.\n",
      "\n",
      "Eleanor, an accomplished violinist, took to the stage with a grace born of years of practice, her fingers dancing over the strings as she poured her soul into the performance, captivating the audience with a melody that spoke of love, loss, and redemption.\n",
      "\n",
      "Amidst the chaos of the battlefield, the young soldier found a moment of clarity, realizing the true cost of war not in terms of territory gained or lost, but in the human lives irrevocably changed by the horrors they had witnessed and the sacrifices they had made.\n",
      "\n",
      "The chef, a master of his craft, meticulously combined the freshest local ingredients with exotic spices brought from distant lands, creating a dish that was not only a feast for the palate but also a celebration of the diverse cultures that had influenced his culinary journey.\n",
      "\n",
      "As the debate raged on, the professor, armed with decades of research and an unwavering belief in the importance of preserving ancient languages, defended their value not merely as academic curiosities but as vital links to our collective human heritage and windows into the minds of our ancestors.\n",
      "\n",
      "The novelist, after years of grappling with writer's block, found inspiration in the least expected of places—a small, unremarkable café where the mosaic of human interactions and the simplicity of everyday moments sparked the idea for her next best-selling novel, weaving together themes of love, resilience, and the beauty of the mundane.\n"
     ]
    }
   ],
   "source": [
    "result = search(sentences,\"manuscripts\")\n",
    "print(result)\n",
    "for i in result:\n",
    "    print()\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
