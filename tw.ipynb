{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "0.16070553380118568\n",
      "---\n",
      "0.0\n",
      "---\n",
      "0.281234684152075\n",
      "---\n",
      "0.3749795788694333\n"
     ]
    }
   ],
   "source": [
    "\n",
    "document1 = \"Machine learning teaches machine how to learn\"\n",
    "document2 = \"i am apple\"\n",
    "document3 = \"mango i like learning\"\n",
    "document4 = \"learning is shit\"\n",
    "docs=[document1,document2,document3,document4]\n",
    "\n",
    "def termFrequency(term, document):\n",
    "    normalizeDocument = document.lower().split()\n",
    "    # print(normalizeDocument)\n",
    "    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\n",
    "   \n",
    "# IDF of a term\n",
    "def inverseDocumentFrequency(term, documents):\n",
    "    count = 0\n",
    "    for doc in documents:\n",
    "        if term.lower() in doc.lower().split():\n",
    "            # print(doc.lower().split())\n",
    "\n",
    "            count += 1\n",
    "    if count > 0:\n",
    "        # print(len(documents),count,float(len(documents))/count,math.log(float(len(documents))/count,10))\n",
    "\n",
    "        return 1.0 + math.log(float(len(documents))/count,10)\n",
    "\n",
    "    else:\n",
    "        return 1.0\n",
    "        \n",
    "# tf-idf of a term in a document\n",
    "def tf_idf(term, document, documents):\n",
    "    tf = termFrequency(term, document)\n",
    "    idf = inverseDocumentFrequency(term, documents)\n",
    "    # print(tf,idf)\n",
    "    return tf*idf\n",
    "for i in docs:\n",
    "    print(\"---\")\n",
    "    print(tf_idf(\"learning\",i,docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents1221=[]\n",
    "# bm25=BM25Okapi()\n",
    "def tokenize_documents( documents):\n",
    "    \"\"\"Tokenize and prepare documents for BM25.\"\"\"\n",
    "    tokenized_docs = [tokenizer.tokenize(doc) for doc in documents]\n",
    "    # documents1221 = tokenized_docs\n",
    "    bm25 = BM25Okapi(tokenized_docs)\n",
    "    # print(tokenized_docs)\n",
    "\n",
    "# def predict_term_weights(self, query):\n",
    "#     \"\"\"Predict term weights for the given query.\"\"\"\n",
    "#     # Tokenize query\n",
    "#     query_tokens = self.tokenizer.tokenize(query)\n",
    "#     query_ids = self.tokenizer.convert_tokens_to_ids(query_tokens)\n",
    "    # print(bm25.doc_freqs)\n",
    "    return bm25\n",
    "\n",
    "def predict_term_weights( query):\n",
    "    \"\"\"Predict term weights for the given query using BERT embeddings.\"\"\"\n",
    "    inputs = tokenizer(query, return_tensors=\"pt\")\n",
    "    # print(inputs)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        # Use the last hidden state\n",
    "        last_hidden_states = outputs.last_hidden_state\n",
    "    # print(len(last_hidden_states[0]))\n",
    "    # print((last_hidden_states))\n",
    "    # Simplified weight prediction logic: Use the norm of the embeddings as weights\n",
    "    # This is a placeholder and should be replaced with a proper mechanism\n",
    "    weights = torch.norm(last_hidden_states, dim=-1).squeeze().tolist()\n",
    "    # print(weights)\n",
    "\n",
    "    # Associate weights with tokens. This assumes no special tokens (CLS, SEP) for simplicity.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"].squeeze().tolist())\n",
    "    return dict(zip(tokens, weights))\n",
    "# print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'[CLS]': 14.703083038330078,\n",
       " 'machine': 15.307113647460938,\n",
       " 'learning': 14.563008308410645,\n",
       " 'teaches': 14.735929489135742,\n",
       " 'how': 16.826915740966797,\n",
       " 'to': 15.747702598571777,\n",
       " 'learn': 15.463006019592285,\n",
       " '[SEP]': 15.313431739807129}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for i in docs:\n",
    "    # documents1221 = tokenized_docs\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in docs]\n",
    "    #\n",
    "bm25 = BM25Okapi(tokenized_docs)\n",
    "predict_term_weights(\"Machine learning teaches machine how to learn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from collections import defaultdict\n",
    "def search( query):\n",
    "    \"\"\"Perform a search by optimizing query-document matching scores.\"\"\"\n",
    "    term_weights = predict_term_weights(query)\n",
    "    weighted_query = [(term, weight) for term, weight in term_weights.items()]\n",
    " \n",
    "\n",
    "    # # Calculate scores using BM25 + term weights (simplified version)\n",
    "    scores = defaultdict(float)\n",
    "    # print(scores.items())\n",
    "    # # print(\"sssss\",weighted_query)\n",
    "    \n",
    "    for term, weight in weighted_query:\n",
    "        term_scores = bm25.get_scores([term])\n",
    "        # print(term_scores)\n",
    "        for doc_id, score in enumerate(term_scores):\n",
    "            # print(doc_id,score)\n",
    "            scores[doc_id] += score * weight\n",
    "        # print(scores.items())\n",
    "    # # Sort documents by their score\n",
    "        sorted_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
    "        return [docs[doc_id] for doc_id, _ in sorted_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Machine learning teaches machine how to learn',\n",
       " 'i am apple',\n",
       " 'mango i like learning',\n",
       " 'learning is shit']"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(\"i like mango\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n"
     ]
    }
   ],
   "source": [
    "# for i in docs:\n",
    "print(termFrequency(\"machine\",document2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine: 1.0\n",
      "learning: 1.2876820724517808\n",
      "teaches: 1.0\n",
      "machine: 1.0\n",
      "how: 1.0\n",
      "to: 1.0\n",
      "learn: 1.0\n",
      "---------\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "Machine: 0.5996035110480314\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "learning: 0.2998017555240157\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "teaches: 0.2998017555240157\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "machine: 0.5996035110480314\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "how: 0.2998017555240157\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "to: 0.2998017555240157\n",
      "['machine', 'learning', 'teaches', 'machine', 'how', 'to', 'learn']\n",
      "learn: 0.2998017555240157\n"
     ]
    }
   ],
   "source": [
    "documents = [  \n",
    "    \"Machine learning is a subset of artificial intelligence.\",\n",
    "    \"Deep learning is a type of machine learning.\",\n",
    "    \"Natural language processing is a part of machine learning.\",\n",
    "    \"Machine learning algorithms improve over time through learning from data.\"]\n",
    "for term in document1.split():\n",
    "    print(\"{}: {}\".format(term, inverseDocumentFrequency(term, documents)))\n",
    " \n",
    "print(\"---------\")\n",
    "# tf-idf test run:\n",
    "documents = [document1, document2, document3]\n",
    "for term in document1.split(\" \"):\n",
    "    print(\"{}: {}\".format(term, tf_idf(term, document1, documents)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/praveenlawyantra/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import math \n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'data', 'is', 'Science', 'the', 'machine', 'of', 'job', 'key', 'for', 'learning', 'science', 'Data', '21st', 'sexiest', 'century'}\n"
     ]
    }
   ],
   "source": [
    "first_sentence = \"Data Science is the sexiest job of the 21st century\"\n",
    "second_sentence = \"machine learning is the key for data science\"\n",
    "#split so each word have their own string\n",
    "first_sentence = first_sentence.split(\" \")\n",
    "second_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\n",
    "total= set(first_sentence).union(set(second_sentence))\n",
    "print(total)\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# stop_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data', 'Science', 'machine', 'job', 'key', 'learning', 'science', 'Data', '21st', 'sexiest', 'century']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>is</th>\n",
       "      <th>Science</th>\n",
       "      <th>the</th>\n",
       "      <th>machine</th>\n",
       "      <th>of</th>\n",
       "      <th>job</th>\n",
       "      <th>key</th>\n",
       "      <th>for</th>\n",
       "      <th>learning</th>\n",
       "      <th>science</th>\n",
       "      <th>Data</th>\n",
       "      <th>21st</th>\n",
       "      <th>sexiest</th>\n",
       "      <th>century</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   data  is  Science  the  machine  of  job  key  for  learning  science   \n",
       "0     0   1        1    2        0   1    1    0    0         0        0  \\\n",
       "1     1   1        0    1        1   0    0    1    1         1        1   \n",
       "\n",
       "   Data  21st  sexiest  century  \n",
       "0     1     1        1        1  \n",
       "1     0     0        0        0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordDictA = dict.fromkeys(total, 0) \n",
    "wordDictB = dict.fromkeys(total, 0)\n",
    "for word in first_sentence:\n",
    "    wordDictA[word]+=1\n",
    "    \n",
    "for word in second_sentence:\n",
    "    wordDictB[word]+=1\n",
    "\n",
    "filtered_sentence = [w for w in wordDictA if not w in stop_words]\n",
    "print(filtered_sentence)\n",
    "pd.DataFrame([wordDictA, wordDictB])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sexiest</th>\n",
       "      <th>is</th>\n",
       "      <th>Science</th>\n",
       "      <th>job</th>\n",
       "      <th>century</th>\n",
       "      <th>science</th>\n",
       "      <th>for</th>\n",
       "      <th>data</th>\n",
       "      <th>Data</th>\n",
       "      <th>key</th>\n",
       "      <th>of</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>21st</th>\n",
       "      <th>the</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sexiest     is  Science  job  century  science    for   data  Data    key   \n",
       "0      0.1  0.100      0.1  0.1      0.1    0.000  0.000  0.000   0.1  0.000  \\\n",
       "1      0.0  0.125      0.0  0.0      0.0    0.125  0.125  0.125   0.0  0.125   \n",
       "\n",
       "    of  learning  machine  21st    the  \n",
       "0  0.1     0.000    0.000   0.1  0.200  \n",
       "1  0.0     0.125    0.125   0.0  0.125  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def computeTF(wordDict, doc):\n",
    "    tfDict = {}\n",
    "    corpusCount = len(doc)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count/float(corpusCount)\n",
    "    return(tfDict)\n",
    "#running our sentences through the tf function:\n",
    "tfFirst = computeTF(wordDictA, first_sentence)\n",
    "tfSecond = computeTF(wordDictB, second_sentence)\n",
    "#Converting to dataframe for visualization\n",
    "tf = pd.DataFrame([tfFirst, tfSecond])\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeIDF(docList):\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for word, val in idfDict.items():\n",
    "        idfDict[word] = math.log10(N / (float(val) + 1))\n",
    "        \n",
    "    return(idfDict)\n",
    "#inputing our sentences in the log file\n",
    "idfs = computeIDF([wordDictA, wordDictB])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    sexiest        is   Science       job   century   science       for   \n",
      "0  0.030103  0.030103  0.030103  0.030103  0.030103  0.000000  0.000000  \\\n",
      "1  0.000000  0.037629  0.000000  0.000000  0.000000  0.037629  0.037629   \n",
      "\n",
      "       data      Data       key        of  learning   machine      21st   \n",
      "0  0.000000  0.030103  0.000000  0.030103  0.000000  0.000000  0.030103  \\\n",
      "1  0.037629  0.000000  0.037629  0.000000  0.037629  0.037629  0.000000   \n",
      "\n",
      "        the  \n",
      "0  0.060206  \n",
      "1  0.037629  \n"
     ]
    }
   ],
   "source": [
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val*idfs[word]\n",
    "    return(tfidf)\n",
    "#running our two sentences through the IDF:\n",
    "idfFirst = computeTFIDF(tfFirst, idfs)\n",
    "idfSecond = computeTFIDF(tfSecond, idfs)\n",
    "#putting it in a dataframe\n",
    "idf= pd.DataFrame([idfFirst, idfSecond])\n",
    "print(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
